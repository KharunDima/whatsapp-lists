"""
–ü–æ–∏—Å–∫ –¥–æ–º–µ–Ω–æ–≤ —á–µ—Ä–µ–∑ CRT.SH - –ø–∞—Ä—Å–∏–Ω–≥ HTML –≤–µ—Ä—Å–∏—è
"""
import aiohttp
import asyncio
import logging
import re
import random
from typing import Set
from urllib.parse import quote
from bs4 import BeautifulSoup

from .base import BaseSource

logger = logging.getLogger(__name__)


class CRTShSource(BaseSource):
    """–ò—Å—Ç–æ—á–Ω–∏–∫ –¥–æ–º–µ–Ω–æ–≤ –∏–∑ CRT.SH —Å –ø–∞—Ä—Å–∏–Ω–≥–æ–º HTML"""
    
    def __init__(self, target_config):
        super().__init__(target_config)
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        ]
    
    async def get_domains(self) -> Set[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç –¥–æ–º–µ–Ω—ã –∏–∑ CRT.SH —á–µ—Ä–µ–∑ HTML –ø–∞—Ä—Å–∏–Ω–≥"""
        domains = set()
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ (–∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç—ã–µ, –±–µ–∑ %)
        patterns = [
            ("whatsapp.net", "whatsapp.net"),
            ("whatsapp.com", "whatsapp.com"),
            ("wa.me", "wa.me"),
            ("fbcdn.net", "fbcdn.net"),
            ("facebook.com", "facebook.com"),
            ("fb.com", "fb.com"),
        ]
        
        for name, pattern in patterns:
            try:
                logger.info(f"üåê CRT.SH HTML: {name}")
                pattern_domains = await self._query_crt_sh_html(pattern)
                
                if pattern_domains:
                    domains.update(pattern_domains)
                    logger.info(f"   –ù–∞–π–¥–µ–Ω–æ: {len(pattern_domains)} –¥–æ–º–µ–Ω–æ–≤")
                    
                    # –ü–æ–∫–∞–∂–µ–º –ø—Ä–∏–º–µ—Ä—ã
                    if len(pattern_domains) > 0:
                        sample = list(pattern_domains)[:3]
                        for domain in sample:
                            logger.debug(f"     ‚Ä¢ {domain}")
                else:
                    logger.debug(f"   –ù–µ –Ω–∞–π–¥–µ–Ω–æ –¥–æ–º–µ–Ω–æ–≤")
                
                # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                await asyncio.sleep(random.uniform(5, 8))
            
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –¥–ª—è '{name}': {str(e)[:100]}")
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è
        filtered = self._filter_domains(domains)
        logger.info(f"‚úÖ CRT.SH: –≤—Å–µ–≥–æ {len(filtered)} –¥–æ–º–µ–Ω–æ–≤ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏")
        return filtered
    
    async def _query_crt_sh_html(self, pattern: str, timeout: int = 60) -> Set[str]:
        """–ó–∞–ø—Ä–æ—Å –∫ CRT.SH —Å –ø–∞—Ä—Å–∏–Ω–≥–æ–º HTML"""
        domains = set()
        
        # URL –¥–ª—è HTML –ø–æ–∏—Å–∫–∞ (–±–µ–∑ &output=json)
        url = f"https://crt.sh/?q={quote(pattern)}"
        headers = {
            "User-Agent": random.choice(self.user_agents),
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.5",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "Sec-Fetch-Dest": "document",
            "Sec-Fetch-Mode": "navigate",
            "Sec-Fetch-Site": "none",
            "Sec-Fetch-User": "?1",
            "Cache-Control": "max-age=0",
        }
        
        try:
            connector = aiohttp.TCPConnector(ssl=False)
            async with aiohttp.ClientSession(connector=connector) as session:
                async with session.get(url, headers=headers, timeout=timeout) as response:
                    
                    logger.debug(f"   –°—Ç–∞—Ç—É—Å: {response.status}")
                    
                    if response.status == 200:
                        html = await response.text()
                        
                        # –ü–∞—Ä—Å–∏–º HTML —Å BeautifulSoup
                        soup = BeautifulSoup(html, 'html.parser')
                        
                        # –ú–µ—Ç–æ–¥ 1: –ò—â–µ–º –≤ —Ç–∞–±–ª–∏—Ü–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                        table_domains = self._parse_results_table(soup)
                        domains.update(table_domains)
                        
                        # –ú–µ—Ç–æ–¥ 2: –ò—â–µ–º –≤–æ –≤—Å–µ–º —Ç–µ–∫—Å—Ç–µ
                        text_domains = self._extract_domains_from_text(html)
                        domains.update(text_domains)
                        
                        # –ú–µ—Ç–æ–¥ 3: –ò—â–µ–º —Å—Å—ã–ª–∫–∏
                        link_domains = self._parse_links(soup)
                        domains.update(link_domains)
                        
                        logger.debug(f"   HTML –ø–∞—Ä—Å–∏–Ω–≥: –Ω–∞–π–¥–µ–Ω–æ {len(domains)} –¥–æ–º–µ–Ω–æ–≤")
                    
                    elif response.status == 429:
                        logger.warning(f"   Rate limit, –∂–¥–µ–º 20 —Å–µ–∫—É–Ω–¥...")
                        await asyncio.sleep(20)
                    elif response.status == 502 or response.status == 503:
                        logger.warning(f"   –°–µ—Ä–≤–∏—Å –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω (HTTP {response.status})")
                    else:
                        logger.warning(f"   HTTP {response.status}")
        
        except asyncio.TimeoutError:
            logger.warning(f"   –¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞")
        except aiohttp.ClientError as e:
            logger.warning(f"   –û—à–∏–±–∫–∞ –∫–ª–∏–µ–Ω—Ç–∞: {e}")
        except Exception as e:
            logger.debug(f"   –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {e}")
        
        return domains
    
    def _parse_results_table(self, soup) -> Set[str]:
        """–ü–∞—Ä—Å–∏—Ç —Ç–∞–±–ª–∏—Ü—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ CRT.SH"""
        domains = set()
        
        try:
            # –ò—â–µ–º —Ç–∞–±–ª–∏—Ü—É —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
            tables = soup.find_all('table')
            
            for table in tables:
                # –ò—â–µ–º —Å—Ç—Ä–æ–∫–∏ —Ç–∞–±–ª–∏—Ü—ã
                rows = table.find_all('tr')
                
                for row in rows:
                    # –ò—â–µ–º —è—á–µ–π–∫–∏ —Å –¥–æ–º–µ–Ω–∞–º–∏
                    cells = row.find_all('td')
                    
                    for cell in cells:
                        text = cell.get_text(strip=True)
                        if text:
                            # –û—á–∏—â–∞–µ–º –∏ –¥–æ–±–∞–≤–ª—è–µ–º –¥–æ–º–µ–Ω
                            domain = self._clean_domain(text)
                            if domain and self._is_potential_target_domain(domain):
                                domains.add(domain)
                            
                            # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å—Å—ã–ª–∫–∏ –≤ —è—á–µ–π–∫–µ
                            links = cell.find_all('a')
                            for link in links:
                                href = link.get('href', '')
                                if href and '?id=' in href:
                                    # –≠—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å —Å—Å—ã–ª–∫–∞ –Ω–∞ —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç
                                    link_text = link.get_text(strip=True)
                                    if link_text:
                                        domain = self._clean_domain(link_text)
                                        if domain and self._is_potential_target_domain(domain):
                                            domains.add(domain)
        
        except Exception as e:
            logger.debug(f"   –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ç–∞–±–ª–∏—Ü—ã: {e}")
        
        return domains
    
    def _extract_domains_from_text(self, html: str) -> Set[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–æ–º–µ–Ω—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞ HTML"""
        domains = set()
        
        if not html:
            return domains
        
        # –£–±–∏—Ä–∞–µ–º HTML —Ç–µ–≥–∏ –¥–ª—è —á–∏—Å—Ç–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        text = re.sub(r'<[^>]+>', ' ', html)
        
        # –ò—â–µ–º –¥–æ–º–µ–Ω—ã –≤ —Ç–µ–∫—Å—Ç–µ
        # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –¥–æ–º–µ–Ω–æ–≤
        domain_pattern = r'\b(?:[a-zA-Z0-9](?:[a-zA-Z0-9\-]{0,61}[a-zA-Z0-9])?\.)+[a-zA-Z]{2,}\b'
        matches = re.findall(domain_pattern, text)
        
        for match in matches:
            cleaned = self._clean_domain(match)
            if cleaned and self._is_potential_target_domain(cleaned):
                domains.add(cleaned)
        
        return domains
    
    def _parse_links(self, soup) -> Set[str]:
        """–ü–∞—Ä—Å–∏—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ"""
        domains = set()
        
        try:
            links = soup.find_all('a')
            
            for link in links:
                href = link.get('href', '')
                text = link.get_text(strip=True)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º URL —Å—Å—ã–ª–∫–∏
                if href:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–æ–º–µ–Ω –∏–∑ URL
                    domain_from_href = self._extract_domain_from_url(href)
                    if domain_from_href and self._is_potential_target_domain(domain_from_href):
                        domains.add(domain_from_href)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–∫—Å—Ç —Å—Å—ã–ª–∫–∏
                if text:
                    domain_from_text = self._clean_domain(text)
                    if domain_from_text and self._is_potential_target_domain(domain_from_text):
                        domains.add(domain_from_text)
        
        except Exception as e:
            logger.debug(f"   –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Å—ã–ª–æ–∫: {e}")
        
        return domains
    
    def _extract_domain_from_url(self, url: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–æ–º–µ–Ω –∏–∑ URL"""
        if not url:
            return ""
        
        # –£–±–∏—Ä–∞–µ–º –ø—Ä–æ—Ç–æ–∫–æ–ª
        url = re.sub(r'^https?://', '', url)
        
        # –£–±–∏—Ä–∞–µ–º –ø—É—Ç—å –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        url = url.split('/')[0]
        url = url.split('?')[0]
        url = url.split('#')[0]
        
        # –£–±–∏—Ä–∞–µ–º –ø–æ—Ä—Ç
        url = url.split(':')[0]
        
        return self._clean_domain(url)
    
    def _is_potential_target_domain(self, domain: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –º–æ–∂–µ—Ç –ª–∏ –¥–æ–º–µ–Ω –±—ã—Ç—å —Ü–µ–ª—å—é"""
        if not domain:
            return False
        
        domain_lower = domain.lower()
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ Facebook/WhatsApp
        target_keywords = [
            'whatsapp', 'wa', 'facebook', 'fb', 'meta', 'fbcdn',
            'instagram', 'oculus', 'threads', 'workplace'
        ]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        for keyword in target_keywords:
            if keyword in domain_lower:
                return True
        
        # –¢–∞–∫–∂–µ —Ä–∞–∑—Ä–µ—à–∞–µ–º –¥–æ–º–µ–Ω—ã —Å –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        infra_patterns = [
            r'\.fna\.(whatsapp\.net|fbcdn\.net)$',
            r'f[a-z]{3}\d+-\d+\.fna\.',
            r'[scme]\d+\.whatsapp\.net$',
            r'node\d+\.whatsapp\.net$',
            r'edge\d+\.whatsapp\.net$',
            r'server\d+\.whatsapp\.net$',
            r'bsg\d+\.whatsapp\.net$',
        ]
        
        for pattern in infra_patterns:
            if re.search(pattern, domain_lower):
                return True
        
        return False
    
    def _clean_domain(self, domain: str) -> str:
        """–û—á–∏—â–∞–µ—Ç –¥–æ–º–µ–Ω"""
        if not domain:
            return ""
        
        domain = domain.lower().strip()
        
        # –£–¥–∞–ª—è–µ–º wildcard
        if domain.startswith('*.'):
            domain = domain[2:]
        
        # –£–¥–∞–ª—è–µ–º –ø—Ä–æ—Ç–æ–∫–æ–ª—ã
        if domain.startswith(('http://', 'https://')):
            domain = domain.split('://')[1]
        
        # –£–¥–∞–ª—è–µ–º –ø–æ—Ä—Ç
        if ':' in domain:
            domain = domain.split(':')[0]
        
        # –£–¥–∞–ª—è–µ–º –ø—É—Ç—å
        if '/' in domain:
            domain = domain.split('/')[0]
        
        # –£–¥–∞–ª—è–µ–º –∫–∞–≤—ã—á–∫–∏ –∏ –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã
        domain = domain.strip('"\'`<>[]()')
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å
        if not self._is_valid_domain(domain):
            return ""
        
        return domain
    
    def _is_valid_domain(self, domain: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å –¥–æ–º–µ–Ω–∞"""
        if not domain or len(domain) < 4 or len(domain) > 253:
            return False
        
        # –î–æ–ª–∂–Ω–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Ç–æ—á–∫—É
        if '.' not in domain:
            return False
        
        # –ù–µ IP –∞–¥—Ä–µ—Å
        if re.match(r'^\d+\.\d+\.\d+\.\d+$', domain):
            return False
        
        # –î–≤–æ–π–Ω—ã–µ –¥–µ—Ñ–∏—Å—ã –∏–ª–∏ —Ç–æ—á–∫–∏
        if '--' in domain or '..' in domain:
            return False
        
        # –ù–µ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è/–∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –¥–µ—Ñ–∏—Å–æ–º –∏–ª–∏ —Ç–æ—á–∫–æ–π
        if domain.startswith(('-', '.')) or domain.endswith(('-', '.')):
            return False
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ TLD
        parts = domain.split('.')
        if len(parts) < 2:
            return False
        
        tld = parts[-1]
        if len(tld) < 2 or len(tld) > 10:
            return False
        
        return True
    
    def _filter_domains(self, domains: Set[str]) -> Set[str]:
        """–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–æ–º–µ–Ω–æ–≤"""
        filtered = set()
        
        for domain in domains:
            domain_lower = domain.lower()
            
            # –ë—ã—Å—Ç—Ä—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏
            if not domain_lower or len(domain_lower) < 4:
                continue
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —è–≤–Ω—ã–π –º—É—Å–æ—Ä
            if self._is_garbage_domain(domain_lower):
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å–∫–ª—é—á–∞—é—â–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            if self._matches_exclude(domain_lower):
                continue
            
            # –î–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–ª–∏ –±—ã—Ç—å –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π
            if self._is_potential_target_domain(domain_lower):
                filtered.add(domain_lower)
        
        return filtered
    
    def _is_garbage_domain(self, domain: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –¥–æ–º–µ–Ω –º—É—Å–æ—Ä–æ–º"""
        garbage_patterns = [
            r'\.click$',
            r'\.gratis$',
            r'\.free$',
            r'\.download$',
            r'\.apk$',
            r'cloudflaressl\.com$',
            r'\.tk$|\.ml$|\.ga$|\.cf$|\.gq$|\.xyz$|\.top$|\.club$|\.site$|\.online$|\.info$',
            r'whatsapp.*whatsapp',  # –ü–æ–≤—Ç–æ—Ä–µ–Ω–∏—è
            r'descargar.*whatsapp',
            r'baixar.*whatsapp',
            r'kostenlos.*whatsapp',
            r'--\d+',  # –î–≤–æ–π–Ω—ã–µ –¥–µ—Ñ–∏—Å—ã —Å —á–∏—Å–ª–∞–º–∏
        ]
        
        for pattern in garbage_patterns:
            if re.search(pattern, domain, re.IGNORECASE):
                return True
        
        return False